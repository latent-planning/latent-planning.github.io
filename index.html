<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Learning from Reward-Free Offline Data: A Case for Planning with Latent Dynamics Models">
  <meta name="keywords" content="Offline reinforcement learning, goal-conditioned reinforcement learning,optimal control,latent dynamics,planning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Learning from Reward-Free Offline Data: A Case for Planning with Latent Dynamics Models</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!--<script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>-->

  <!-- Google Tag Manager -->
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
    new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
    j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
    'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
    })(window,document,'script','dataLayer','GTM-WW9Q7B6L');</script>
    <!-- End Google Tag Manager -->

  <!--<script>-->
  <!--  window.dataLayer = window.dataLayer || [];-->

  <!--  function gtag() {-->
  <!--    dataLayer.push(arguments);-->
  <!--  }-->

  <!--  gtag('js', new Date());-->

  <!--  gtag('config', 'G-PYVRSFMDRL');-->
  <!--</script>-->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <!--<script src="./static/js/index.js"></script>-->
  <script>
    document.addEventListener("DOMContentLoaded", () => {
      const trigger = document.getElementById("collapsible-trigger");
      const collapsibleBody = document.getElementById("collapsible-body");
      const ellipsis = document.getElementById("ellipsis");

      // On click, toggle hidden class & toggle ellipsis
      trigger.addEventListener("click", () => {
        collapsibleBody.classList.toggle("is-hidden");

        if (collapsibleBody.classList.contains("is-hidden")) {
          // Collapsed
          ellipsis.textContent = "... (click to expand)";
        } else {
          // Expanded
          ellipsis.textContent = "";
        }
      });
    });
   </script>
</head>
<body>
<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-WW9Q7B6L"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->

<!--<nav class="navbar" role="navigation" aria-label="main navigation">-->
<!--  <div class="navbar-brand">-->
<!--    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">-->
<!--      <span aria-hidden="true"></span>-->
<!--      <span aria-hidden="true"></span>-->
<!--      <span aria-hidden="true"></span>-->
<!--    </a>-->
<!--  </div>-->
<!--  <div class="navbar-menu">-->
<!--    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">-->
<!--      <a class="navbar-item" href="https://vladisai.github.io">-->
<!--      <span class="icon">-->
<!--          <i class="fas fa-home"></i>-->
<!--      </span>-->
<!--      </a>-->
<!--    </div>-->
<!--  </div>-->
<!--</nav>-->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Learning from Reward-Free Offline Data:<br/> A Case for Planning with<br/> Latent Dynamics Models</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://vladisai.github.io">Vlad Sobal</a><sup>*1</sup>,</span>
            <span class="author-block">
              <a href="https://kevinghst.github.io/">Wancong Zhang</a><sup>*1</sup>,</span>
            <span class="author-block">
              <a href="https://kyunghyuncho.me/">Kyunghyun Cho</a><sup>1,2</sup>,
            </span>
            <span class="author-block">
              <a href="https://randallbalestriero.github.io/">Randall Balestriero</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="https://timrudner.com/">Tim G. J. Rudner</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="http://yann.lecun.com/">Yann LeCun</a><sup>1,4</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>New York University,</span>
            <span class="author-block"><sup>2</sup>Genentech,</span>
            <span class="author-block"><sup>3</sup>Brown University,</span>
            <span class="author-block"><sup>4</sup>Meta FAIR</span><br/>
            <span class="author-block"><sup>*</sup>Equal contribution, order determined by coin flip,</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!--[> Video Link. <]-->
              <!--<span class="link-block">-->
              <!--  <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"-->
              <!--     class="external-link button is-normal is-rounded is-dark">-->
              <!--    <span class="icon">-->
              <!--        <i class="fab fa-youtube"></i>-->
              <!--    </span>-->
              <!--    <span>Video</span>-->
              <!--  </a>-->
              <!--</span>-->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>

              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://github.com/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!--<section class="section">-->
<!--  <div class="container is-max-desktop">-->
<!--    <div class="columns is-centered has-text-centered">-->
<!--      <div class="column content">-->
<!--        <h2 class="title is-5">Overview figure</h2>-->
<!--        <figure class="image is-fullwidth">-->
<!--          <img src="./static/images/main_idea.png" alt="Overview figure">-->
<!--        </figure>-->
<!--        Test title.-->
<!--      </div>-->
<!--    </div>-->
<!--  </div>-->
<!--</section>-->

<section class="section">
  <div class="container is-max-dextop">
    <div class="columns is-centered has-text-centered">
      <div class="column content">
        <h2 class="title is-3">Overview</h2>
        <figure class="image is-fullwidth">
          <img src="./static/images/main_idea.png" alt="Overview of our analysis">
          <figcaption class="is-size-5">
            <strong>Overview of our analysis.</strong> We test six methods for learning from offline reward-free
            trajectories on 23 different datasets across two top-down navigation environments. We evaluate for six generalization properties required to scale to large offline datasets of suboptimal trajectories. We find that planning with a latent dynamics model (PLDM) demonstrates the highest level of generalization. For a full comparison, see Table 1. <strong>Right:</strong> Diagram of PLDM. Circles represent variables, rectangles—loss components, half-ovals—trained models.
          </figcaption>
        </figure>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
          A long-standing goal in AI is to build agents that can solve a variety of tasks across different environments, including previously unseen ones. Two dominant approaches tackle this challenge: (i) reinforcement learning (RL), which learns policies through trial and error, and (ii) optimal control, which plans actions using a learned or known dynamics model. However, their relative strengths and weaknesses remain underexplored in the setting where agents must learn from offline trajectories without reward annotations.
          </p>
          <p>In this work, we systematically analyze the performance of different RL and control-based methods under datasets of varying quality. On the RL side, we consider goal-conditioned and zero-shot approaches. On the control side, we train a latent dynamics model using the Joint Embedding Predictive Architecture (JEPA) and use it for planning. We study how dataset properties—such as data diversity, trajectory quality, and environment variability—affect the performance of these approaches.</p>
          <p>Our results show that model-free RL excels when abundant, high-quality data is available, while model-based planning excels in generalization to novel environment leayouts, in trajectory stitching, and data-efficiency. Notably, planning with a latent dynamics model emerges as a promising approach for zero-shot generalization from suboptimal data.</p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" id="setting">
  <div class="container is-max-desktop content">
    <h2 class="title is-3">Environments</h2>
    <h3 class="title is-4">The task: reach a specified goal in top-down navigation</h4>
    We present two top-down navigation environments. In both, the goal is to reach a specified goal state. 
    The task is illustrated on the left. We test algorithms' ability to learn from data of varying quality.
    On the right is an example trajectory in the offline dataset.
    <div class="columns is-centered">
      <div class="column is-one-third">
        <figure class="image gif-container-big">
          <img src="./static/trajs/solved_episode.gif" alt="Example trajectory 1">
        </figure>
      </div>
      <div class="column is-one-third">
        <figure class="image" style="width:340px":>
          <img src="./static/trajs/good_ds/episode_4.gif" alt="Example dataset trajectory 4">
        </figure>
      </div>
    </div>
    <div class="content" id="collapsible-trigger" style="cursor: pointer;">
      <h2 class="title is-4">
        Example Trajectories from the Dataset
        <span id="ellipsis" class="is-size-7 has-text-grey">... (click to expand)</span>
      </h2>
    </div>

    <div id="collapsible-body" class="is-hidden">
    <p>
      The following GIFs show example trajectories collected in the dataset. These illustrate the 
      variety of movements and behaviors captured in different data settings.
    </p>
    <h4 class="title is-5">Good quality-trajectories (length=90)</h4>
    <div class="columns is-centered">
      <div class="column is-one-fifth">
        <figure class="image gif-container">
          <img src="./static/trajs/good_ds/episode_0.gif" alt="Example trajectory 1">
        </figure>
      </div>
      <div class="column is-one-fifth">
        <figure class="image gif-container">
          <img src="./static/trajs/good_ds/episode_1.gif" alt="Example trajectory 2">
        </figure>
      </div>
      <div class="column is-one-fifth">
        <figure class="image gif-container">
          <img src="./static/trajs/good_ds/episode_2.gif" alt="Example trajectory 3">
        </figure>
      </div>
      <div class="column is-one-fifth">
        <figure class="image gif-container">
          <img src="./static/trajs/good_ds/episode_3.gif" alt="Example trajectory 4">
        </figure>
      </div>
      <div class="column is-one-fifth">
        <figure class="image gif-container">
          <img src="./static/trajs/good_ds/episode_4.gif" alt="Example trajectory 5">
        </figure>
      </div>
    </div>
    <h4 class="title is-5">Random policy trajectories</h4>
    <div class="columns is-centered">
      <div class="column is-one-fifth">
        <figure class="image gif-container">
          <img src="./static/trajs/random_ds/episode_0.gif" alt="Example trajectory 1">
        </figure>
      </div>
      <div class="column is-one-fifth">
        <figure class="image gif-container">
          <img src="./static/trajs/random_ds/episode_1.gif" alt="Example trajectory 2">
        </figure>
      </div>
      <div class="column is-one-fifth">
        <figure class="image gif-container">
          <img src="./static/trajs/random_ds/episode_2.gif" alt="Example trajectory 3">
        </figure>
      </div>
      <div class="column is-one-fifth">
        <figure class="image gif-container">
          <img src="./static/trajs/random_ds/episode_3.gif" alt="Example trajectory 4">
        </figure>
      </div>
      <div class="column is-one-fifth">
        <figure class="image gif-container">
          <img src="./static/trajs/random_ds/episode_4.gif" alt="Example trajectory 5">
        </figure>
      </div>
    </div>

    <h4 class="title is-5">Trajectories of length 16</h4>
    <div class="columns is-centered">
      <div class="column is-one-fifth">
        <figure class="image gif-container">
          <img src="./static/trajs/17_ds/episode_0.gif" alt="Example trajectory 1">
        </figure>
      </div>
      <div class="column is-one-fifth">
        <figure class="image gif-container">
          <img src="./static/trajs/17_ds/episode_1.gif" alt="Example trajectory 2">
        </figure>
      </div>
      <div class="column is-one-fifth">
        <figure class="image gif-container">
          <img src="./static/trajs/17_ds/episode_2.gif" alt="Example trajectory 3">
        </figure>
      </div>
      <div class="column is-one-fifth">
        <figure class="image gif-container">
          <img src="./static/trajs/17_ds/episode_3.gif" alt="Example trajectory 4">
        </figure>
      </div>
      <div class="column is-one-fifth">
        <figure class="image gif-container">
          <img src="./static/trajs/17_ds/episode_4.gif" alt="Example trajectory 5">
        </figure>
      </div>
    </div>

    <h4 class="title is-5">Trajectories of length 32</h4>
    <div class="columns is-centered">
      <div class="column is-one-fifth">
        <figure class="image gif-container">
          <img src="./static/trajs/33_ds/episode_0.gif" alt="Example trajectory 1">
        </figure>
      </div>
      <div class="column is-one-fifth">
        <figure class="image gif-container">
          <img src="./static/trajs/33_ds/episode_1.gif" alt="Example trajectory 2">
        </figure>
      </div>
      <div class="column is-one-fifth">
        <figure class="image gif-container">
          <img src="./static/trajs/33_ds/episode_2.gif" alt="Example trajectory 3">
        </figure>
      </div>
      <div class="column is-one-fifth">
        <figure class="image gif-container">
          <img src="./static/trajs/33_ds/episode_3.gif" alt="Example trajectory 4">
        </figure>
      </div>
      <div class="column is-one-fifth">
        <figure class="image gif-container">
          <img src="./static/trajs/33_ds/episode_4.gif" alt="Example trajectory 5">
        </figure>
      </div>
    </div>

    <h4 class="title is-5">Trajectories of length 64</h4>
    <div class="columns is-centered">
      <div class="column is-one-fifth">
        <figure class="image gif-container">
          <img src="./static/trajs/65_ds/episode_0.gif" alt="Example trajectory 1">
        </figure>
      </div>
      <div class="column is-one-fifth">
        <figure class="image gif-container">
          <img src="./static/trajs/65_ds/episode_1.gif" alt="Example trajectory 2">
        </figure>
      </div>
      <div class="column is-one-fifth">
        <figure class="image gif-container">
          <img src="./static/trajs/65_ds/episode_2.gif" alt="Example trajectory 3">
        </figure>
      </div>
      <div class="column is-one-fifth">
        <figure class="image gif-container">
          <img src="./static/trajs/65_ds/episode_3.gif" alt="Example trajectory 4">
        </figure>
      </div>
      <div class="column is-one-fifth">
        <figure class="image gif-container">
          <img src="./static/trajs/65_ds/episode_4.gif" alt="Example trajectory 5">
        </figure>
      </div>
    </div>
    </div>
  </div>
</section>

<!-- Main Results -->
<section class="section" id="results">
  <div class="container is-max-desktop content">
    <h2 class="title is-3">Main Results</h2>
    <div class="columns is-centered has-text-centered">
      <div class="column content">
        <figure class="image is-fullwidth">
          <img src="./static/images/layout_change.png" alt="Maze layout evaluation results">
          <figcaption class="is-size-5">
            <strong>Left:</strong> We train offline goal-conditioned agents on trajectories collected in a subset of maze layouts (left) and evaluate on held-out layouts, observing trajectories shown on the right. Only PLDM solves the task (see Figure 3 for more).  
            <strong>Right:</strong> Success rates of tested methods on held-out layouts, as a function of the number of training layouts. The rightmost plot shows success rates of models trained on data from five layouts, evaluating on held-out layouts ranging from similar to training layouts to out-of-distribution ones. We use map layout edit distance from the training layouts as a measure of distribution shift. PLDM demonstrates the best generalization performance. Results are averaged over 3 seeds, shaded area denotes standard deviation. See Figure 1 for more details on PLDM.
          </figcaption>
        </figure>
      </div>
    </div>
    <p class="has-text-justified">
      We thoroughly evaluated six methods for learning from reward-free offline trajectories. The table below summarizes their performance across several key challenges:
      <em>(i)</em> Transfer to new environments,
      <em>(ii)</em> Zero-shot transfer to a new task,
      <em>(iii)</em> Data-efficiency,
      <em>(iv)</em> Best-case performance when data is abundant and high-quality,
      <em>(v)</em> Ability to learn from random or suboptimal trajectories, and
      <em>(vi)</em> Ability to stitch suboptimal trajectories to solve long-horizon tasks.
    </p>

    <!-- Example: embed a screenshot of your result table -->
    <figure class="image is-fullwidth">
      <img src="./static/images/table.png"
           alt="Comparison table of different methods.">
      <figcaption class="is-size-5">
        <strong>Table&nbsp;1:</strong> Summary of each method's strengths and weaknesses in different data conditions and generalization requirements.
      </figcaption>
    </figure>

    <figure class="image is-fullwidth">
        <img src="./static/images/all_exps.png" alt="Main experiments on two rooms environment.">
        <figcaption class="is-size-5">
            <strong>Testing the selected methods' performance under different dataset constraints.</strong> 
            Values and shaded regions are means and standard deviations over 3 seeds, respectively.
            <br>
            <strong>Left:</strong> To test the importance of the dataset quality, we mix the random policy trajectories 
            and better quality trajectories (see Figure <a href="#fig-traj-examples">X</a>). 
            As the amount of good quality data goes to 0, methods begin to fail, with PLDM and HILP being the most robust ones.
            <br>
            <strong>Center:</strong> We measure methods' performance when trained with different sequence lengths. 
            We find that many goal-conditioned methods fail when train trajectories are short, which causes far-away goals 
            to become out-of-distribution for the resulting policy.
            <br>
            <strong>Right:</strong> We measure methods' performance with datasets of varying sizes. 
            We see that PLDM is the most sample efficient and manages to get almost 50% success rate 
            even with a few thousand transitions.
        </figcaption>
    </figure>

    <h3 class="title is-3">Takeaways</h3>
    <ul>
        <li> Learning a latent dynamics model and using it to plan exhibits robustness to data quality, superior data efficiency, and the best generalization to new layouts and tasks; </li>
        <li> Learning a well-structured latent-space (e.g. using HILP) enables trajectory stitching and robustness to data quality, although it is more data-hungry than other methods; </li>
        <li> Model-free GCRL methods are a great choice when the data is plentiful and good quality. </li>
      <!--<li>-->
      <!--  <strong>Model-free GCRL (HIQL, GCIQL, CRL):</strong> Works extremely well when data coverage is high and trajectories are good-quality. However, these methods struggle when data is sparse, short-horizon, random, or requires stitching.-->
      <!--</li>-->
      <!--<li>-->
      <!--  <strong>HILP:</strong> Performs robustly under poor-quality data and can stitch short segments together. However, it is somewhat more data-hungry and does not generalize as well to new tasks as a learned dynamics model.-->
      <!--</li>-->
      <!--<li>-->
      <!--  <strong>Planning with a Latent Dynamics Model (PLDM):</strong> Shows the best generalization in almost all challenging scenarios—suboptimal data, few samples, new tasks, and new environment layouts—at the cost of somewhat weaker best-case performance (compared to specialized RL) and higher inference-time overhead due to planning.-->
      <!--</li>-->
    </ul>
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{sobal2025learning,
  title={Learning from Reward-Free Offline Data: A Case for Planning with Latent Dynamics Models},
  author={Sobal, Vlad and Zhang, Wancong and Cho, Kynghyun and Balestriero, Randall and Rudner, Tim G. J. and LeCun, Yann},
  journal={arXiv preprint arXiv:XXXX.XXXXX},
  year={2025},
  archivePrefix={arXiv},
}</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/vladisai" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            Based on Nerfies Website (<a href="https://github.com/nerfies/nerfies.github.io">source code</a>).
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
